import gradio as gr
import os

# Suppress PyTorch warnings for cleaner output
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="neucodec")
warnings.filterwarnings("ignore", category=UserWarning, module="torch")

print("â³ Äang khá»Ÿi Ä‘á»™ng VieNeu-TTS...")

# Create output directory on startup
OUTPUT_DIR = "output_audio"
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"ğŸ“ Output folder: {os.path.abspath(OUTPUT_DIR)}")

import soundfile as sf
import tempfile
import torch
from vieneu import VieNeuTTS, FastVieNeuTTS
import sys
import time
import numpy as np
from typing import Generator, Optional, Tuple
import queue
import threading
import yaml
from vieneu_utils.core_utils import split_text_into_chunks, join_audio_chunks, env_bool
from functools import lru_cache
import gc

# --- CONSTANTS & CONFIG ---
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        _config = yaml.safe_load(f) or {}
except Exception as e:
    raise RuntimeError(f"KhÃ´ng thá»ƒ Ä‘á»c config.yaml: {e}")

BACKBONE_CONFIGS = _config.get("backbone_configs", {})
CODEC_CONFIGS = _config.get("codec_configs", {})

_text_settings = _config.get("text_settings", {})
MAX_CHARS_PER_CHUNK = _text_settings.get("max_chars_per_chunk", 256)
MAX_TOTAL_CHARS_STREAMING = _text_settings.get("max_total_chars_streaming", 3000)

if not BACKBONE_CONFIGS or not CODEC_CONFIGS:
    raise ValueError("config.yaml thiáº¿u backbone_configs hoáº·c codec_configs")

# --- 1. MODEL CONFIGURATION ---
# Global model instance
tts = None
current_backbone = None
current_codec = None
model_loaded = False
using_lmdeploy = False

# Cache for reference texts
_ref_text_cache = {}

def get_available_devices() -> list[str]:
    """Get list of available devices for current platform."""
    devices = ["Auto", "CPU"]

    if sys.platform == "darwin":
        # macOS - check MPS
        if torch.backends.mps.is_available():
            devices.append("MPS")
    else:
        # Windows/Linux - check CUDA
        if torch.cuda.is_available():
            devices.append("CUDA")

    return devices

def get_model_status_message() -> str:
    """Reconstruct status message from global state"""
    global model_loaded, tts, using_lmdeploy, current_backbone, current_codec
    if not model_loaded or tts is None:
        return "â³ ChÆ°a táº£i model."
    
    backbone_config = BACKBONE_CONFIGS.get(current_backbone, {})
    codec_config = CODEC_CONFIGS.get(current_codec, {})
    
    backend_name = "ğŸš€ LMDeploy (Optimized)" if using_lmdeploy else "ğŸ“¦ Standard"
    
    # We don't track the exact device strings perfectly in global state, so we estimate
    device_info = "GPU" if using_lmdeploy else "Auto"
    codec_device = "CPU" if "ONNX" in (current_codec or "") else ("GPU/MPS" if torch.cuda.is_available() or torch.backends.mps.is_available() else "CPU")
    
    preencoded_note = "\nâš ï¸ Codec ONNX khÃ´ng há»— trá»£ chá»©c nÄƒng clone giá»ng nÃ³i." if codec_config.get('use_preencoded') else ""
    
    opt_info = ""
    if using_lmdeploy and hasattr(tts, 'get_optimization_stats'):
        stats = tts.get_optimization_stats()
        opt_info = (
            f"\n\nğŸ”§ Tá»‘i Æ°u hÃ³a:"
            f"\n  â€¢ Triton: {'âœ…' if stats['triton_enabled'] else 'âŒ'}"
            f"\n  â€¢ Max Batch Size (Default): {stats.get('max_batch_size', 'N/A')}"
            f"\n  â€¢ Reference Cache: {stats['cached_references']} voices"
            f"\n  â€¢ Prefix Caching: âœ…"
        )

    return (
        f"âœ… Model Ä‘Ã£ táº£i thÃ nh cÃ´ng!\n\n"
        f"ğŸ”§ Backend: {backend_name}\n"
        f"ğŸ¦œ Backbone: {current_backbone}\n"
        f"ğŸµ Codec: {current_codec}{preencoded_note}{opt_info}"
    )

def restore_ui_state():
    """Update UI components based on persistence"""
    global model_loaded
    msg = get_model_status_message()
    return (
        msg, 
        gr.update(interactive=model_loaded), # btn_generate
        gr.update(interactive=False)         # btn_stop
    )

def should_use_lmdeploy(backbone_choice: str, device_choice: str) -> bool:
    """Determine if we should use LMDeploy backend."""
    # LMDeploy not supported on macOS
    if sys.platform == "darwin":
        return False

    if "gguf" in backbone_choice.lower():
        return False

    if device_choice == "Auto":
        has_gpu = torch.cuda.is_available()
    elif device_choice == "CUDA":
        has_gpu = torch.cuda.is_available()
    else:
        has_gpu = False

    return has_gpu

@lru_cache(maxsize=32)
def get_ref_text_cached(text_path: str) -> str:
    """Cache reference text loading"""
    with open(text_path, "r", encoding="utf-8") as f:
        return f.read()

def cleanup_gpu_memory():
    """Aggressively cleanup GPU memory"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    elif torch.backends.mps.is_available():
        torch.mps.empty_cache()
    gc.collect()

def load_model(backbone_choice: str, codec_choice: str, device_choice: str, 
               force_lmdeploy: bool, custom_model_id: str = "", custom_base_model: str = "", 
               custom_hf_token: str = ""):
    """Load model with optimizations and max batch size control"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy
    lmdeploy_error_reason = None
    
    # Check if model is already loaded with same configuration
    if (model_loaded and tts is not None and 
        current_backbone == backbone_choice and 
        current_codec == codec_choice):
        
        success_msg = get_model_status_message()
        
        # Prepare voice update (reuse existing loaded voices)
        try:
            voices = tts.list_preset_voices()
        except Exception:
            voices = []
        
        has_voices = len(voices) > 0
        
        if has_voices:
            default_v = tts._default_voice
            is_tuple = (len(voices) > 0 and isinstance(voices[0], tuple))
            voice_values = [v[1] for v in voices] if is_tuple else voices
            
            if not default_v and voice_values:
                default_v = voice_values[0]
            
            if default_v and default_v not in voice_values:
                if is_tuple:
                    voices.append((default_v, default_v))
                else:
                    voices.append(default_v)
            
            if is_tuple:
                voices.sort(key=lambda x: str(x[0]))
            else:
                voices.sort()
            
            voice_update = gr.update(choices=voices, value=default_v, interactive=True)
            tab_p = gr.update(visible=True)
            tab_c = gr.update(visible=True)
            tab_sel = gr.update(selected="preset_mode")
            mode_state = "preset_mode"
        else:
            msg = "âš ï¸ KhÃ´ng tÃ¬m tháº¥y file voices.json. Vui lÃ²ng dÃ¹ng Tab Voice Cloning."
            voice_update = gr.update(choices=[msg], value=msg, interactive=False)
            tab_p = gr.update(visible=True)
            tab_c = gr.update(visible=True)
            tab_sel = gr.update(selected="preset_mode")
            mode_state = "preset_mode"
        
        yield (
            f"âœ… Model Ä‘Ã£ Ä‘Æ°á»£c táº£i sáºµn!\n\n{success_msg}",
            gr.update(interactive=True),  # btn_generate
            gr.update(interactive=True),  # btn_load
            gr.update(interactive=False), # btn_stop
            voice_update,
            tab_p, tab_c, tab_sel, mode_state
        )
        return
    
    # Otherwise, proceed with normal loading
    model_loaded = False
    
    # Clean up empty token to avoid "Bearer " header issue
    if custom_hf_token is not None and custom_hf_token.strip() == "":
        custom_hf_token = None
    
    yield (
        "â³ Äang táº£i model vá»›i tá»‘i Æ°u hÃ³a... LÆ°u Ã½: QuÃ¡ trÃ¬nh nÃ y sáº½ tá»‘n thá»i gian. Vui lÃ²ng kiÃªn nháº«n.",
        gr.update(interactive=False),
        gr.update(interactive=False),
        gr.update(interactive=False),
        gr.update(),
        gr.update(), gr.update(), gr.update(), gr.update()
    )
    
    try:
        # Cleanup before loading new model
        if tts is not None:
            tts = None # Reset instead of del to avoid NameError if load fails
            cleanup_gpu_memory()
        
        # Prepare Backbone Config/Repo
        custom_loading = False
        is_merged_lora = False

        if backbone_choice == "Custom Model":
            custom_loading = True
            if not custom_model_id or not custom_model_id.strip():
                yield (
                    "âŒ Lá»—i: Vui lÃ²ng nháº­p Model ID cho Custom Model.",
                    gr.update(interactive=False), gr.update(interactive=True), gr.update(interactive=False), gr.update(),
                    gr.update(), gr.update(), gr.update(), gr.update()
                )
                return

            # Check if it is a LoRA to merge
            if "lora" in custom_model_id.lower():
                # Merging mode
                print(f"ğŸ”„ Detected LoRA in name. preparing merge with base: {custom_base_model}")
                if custom_base_model not in BACKBONE_CONFIGS:
                    yield (
                        f"âŒ Lá»—i: Base Model '{custom_base_model}' khÃ´ng há»£p lá»‡.",
                        gr.update(interactive=False), gr.update(interactive=True), gr.update(interactive=False),
                        gr.update(), gr.update(), gr.update(), gr.update(), gr.update()
                    )
                    return
                
                base_config = BACKBONE_CONFIGS[custom_base_model]
                backbone_config = {
                    "repo": base_config["repo"], # Load base first
                    "supports_streaming": base_config["supports_streaming"],
                    "description": f"Custom Merged: {custom_model_id} + {custom_base_model}"
                }
                is_merged_lora = True
            else:
                # Normal custom model
                backbone_config = {
                    "repo": custom_model_id.strip(),
                    "supports_streaming": False, # Assume false for unknown
                    "description": f"Custom Model: {custom_model_id}"
                }
        else:
            backbone_config = BACKBONE_CONFIGS[backbone_choice]
            
        codec_config = CODEC_CONFIGS[codec_choice]
        
        # Override LMDeploy if custom
        if custom_loading:
             if "gguf" in backbone_config['repo'].lower():
                 # GGUF must use Standard backend (llama-cpp)
                 use_lmdeploy = False
             elif is_merged_lora:
                 # LoRA can use LMDeploy if we merge first (checked logic below) or Standard
                 use_lmdeploy = force_lmdeploy and should_use_lmdeploy(custom_base_model, device_choice)
             else:
                 # Full custom model (e.g. finetune)
                 use_lmdeploy = force_lmdeploy and should_use_lmdeploy("VieNeu-TTS (GPU)", device_choice) # Assume GPU compatible?
        else:
             use_lmdeploy = force_lmdeploy and should_use_lmdeploy(backbone_choice, device_choice)
        
        if use_lmdeploy:
            lmdeploy_error_reason = None
            print(f"ğŸš€ Using LMDeploy backend with optimizations")
            
            backbone_device = "cuda"
            
            if "ONNX" in codec_choice:
                codec_device = "cpu"
            else:
                codec_device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # Special handling for Custom LoRA + LMDeploy -> Merge & Save
            target_backbone_repo = backbone_config["repo"]
            
            if custom_loading and is_merged_lora:
                safe_name = custom_model_id.strip().replace("/", "_").replace("\\", "_").replace(":", "")
                cache_dir = os.path.join("merged_models_cache", safe_name)
                target_backbone_repo = os.path.abspath(cache_dir)
                
                # Check if already merged (and voices.json exists)
                if not os.path.exists(cache_dir) or not os.path.exists(os.path.join(cache_dir, "vocab.json")):
                    print(f"ğŸ”„ Merging LoRA for LMDeploy optimization: {cache_dir}")
                    if os.path.exists(cache_dir):
                        print("   âš ï¸ Detected incomplete cache, rebuilding...")
                    yield (
                         f"â³ Äang merge vÃ  lÆ°u model LoRA Ä‘á»ƒ tá»‘i Æ°u cho LMDeploy (thao tÃ¡c nÃ y chá»‰ cháº¡y má»™t láº§n)...",
                         gr.update(interactive=False),
                         gr.update(interactive=False),
                         gr.update(interactive=False),
                         gr.update(),
                         gr.update(), gr.update(), gr.update(), gr.update()
                    )
                    
                    try:
                        # Use GPU for merging if available for speed
                        # We use the Base Model specified
                        base_repo = BACKBONE_CONFIGS[custom_base_model]["repo"]
                        merge_device = "cuda" if torch.cuda.is_available() else "cpu"
                        
                        print(f"   â€¢ Loading base: {base_repo} ({merge_device})")
                        temp_tts = VieNeuTTS(
                            backbone_repo=base_repo,
                            backbone_device=merge_device, 
                            codec_repo=codec_config["repo"],
                            codec_device="cpu", # Codec unused for merging, keep on CPU
                            hf_token=custom_hf_token
                        )
                        
                        print(f"   â€¢ Loading Adapter: {custom_model_id}")
                        temp_tts.load_lora_adapter(custom_model_id.strip(), hf_token=custom_hf_token)
                        
                        print(f"   â€¢ Merging...")
                        if hasattr(temp_tts.backbone, "merge_and_unload"):
                            temp_tts.backbone = temp_tts.backbone.merge_and_unload()
                        
                        print(f"   â€¢ Saving to cache: {cache_dir}")
                        temp_tts.backbone.save_pretrained(cache_dir)
                        temp_tts.tokenizer.save_pretrained(cache_dir)
                        
                        # Fix for LMDeploy: Explicitly save legacy tokenizer files (vocab.json, merges.txt)
                        # because LMDeploy/Transformers might default to slow tokenizer if fast one has issues,
                        # and save_pretrained on fast tokenizer sometimes omits legacy files.
                        try:
                            print("   â€¢ Ensuring legacy tokenizer files...")
                            from transformers import AutoTokenizer
                            slow_tokenizer = AutoTokenizer.from_pretrained(base_repo, use_fast=False)
                            slow_tokenizer.save_pretrained(cache_dir)
                        except Exception as e:
                            print(f"   âš ï¸ Warning: Could not save slow tokenizer files: {e}")

                        # Save voices.json to cache directory so FastVieNeuTTS can find it
                        print(f"   â€¢ Saving voices definition...")
                        import json
                        voices_json_path = os.path.join(cache_dir, "voices.json")
                        voices_content = {
                             "meta": { "note": "Automatically generated during LoRA merge" },
                             "default_voice": temp_tts._default_voice,
                             "presets": temp_tts._preset_voices
                        }
                        with open(voices_json_path, 'w', encoding='utf-8') as f:
                             json.dump(voices_content, f, ensure_ascii=False, indent=2)

                        del temp_tts
                        cleanup_gpu_memory()
                        print("   âœ… Merge & Save successfully!")
                        
                    except Exception as e:
                        import traceback
                        traceback.print_exc()
                        raise RuntimeError(f"Failed to merge & save LoRA for LMDeploy: {e}")

            print(f"ğŸ“¦ Loading optimized model...")
            print(f"   Backbone: {target_backbone_repo} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")
            print(f"   Triton: Enabled")
            
            try:
                tts = FastVieNeuTTS(
                    backbone_repo=target_backbone_repo,
                    backbone_device=backbone_device,
                    codec_repo=codec_config["repo"],
                    codec_device=codec_device,
                    memory_util=0.3,
                    tp=1,
                    enable_prefix_caching=True,
                    enable_triton=True,
                    hf_token=custom_hf_token
                )
                using_lmdeploy = True
                
                # Legacy caching removed
                print(f"   âœ… Optimized backend initialized")
                
            except Exception as e:
                import traceback
                traceback.print_exc()
                
                error_str = str(e)
                if "$env:CUDA_PATH" in error_str:
                    lmdeploy_error_reason = "KhÃ´ng tÃ¬m tháº¥y biáº¿n mÃ´i trÆ°á»ng CUDA_PATH. Vui lÃ²ng cÃ i Ä‘áº·t NVIDIA GPU Computing Toolkit."
                else:
                    lmdeploy_error_reason = f"{error_str}"
                
                yield (
                    f"âš ï¸ LMDeploy Init Error: {lmdeploy_error_reason}. Äang loading model vá»›i backend máº·c Ä‘á»‹nh - tá»‘c Ä‘á»™ cháº­m hÆ¡n so vá»›i lmdeploy...",
                    gr.update(interactive=False),
                    gr.update(interactive=False),
                    gr.update(interactive=False),
                    gr.update(),
                    gr.update(), gr.update(), gr.update(), gr.update()
                )
                time.sleep(1)
                use_lmdeploy = False
                using_lmdeploy = False
        
        if not use_lmdeploy:
            print(f"ğŸ“¦ Using original backend")

            if device_choice == "Auto":
                if "gguf" in backbone_config['repo'].lower():
                    # GGUF: uses Metal on Mac, CUDA on Windows/Linux
                    if sys.platform == "darwin":
                        backbone_device = "gpu"  # llama-cpp-python uses Metal
                    else:
                        backbone_device = "gpu" if torch.cuda.is_available() else "cpu"
                else:
                    # PyTorch model
                    if sys.platform == "darwin":
                        backbone_device = "mps" if torch.backends.mps.is_available() else "cpu"
                    else:
                        backbone_device = "cuda" if torch.cuda.is_available() else "cpu"

                # Codec device
                if "ONNX" in codec_choice:
                    codec_device = "cpu"
                elif sys.platform == "darwin":
                    codec_device = "mps" if torch.backends.mps.is_available() else "cpu"
                else:
                    codec_device = "cuda" if torch.cuda.is_available() else "cpu"

            elif device_choice == "MPS":
                backbone_device = "mps"
                codec_device = "mps" if "ONNX" not in codec_choice else "cpu"

            else:
                backbone_device = device_choice.lower()
                codec_device = device_choice.lower()

                if "ONNX" in codec_choice:
                    codec_device = "cpu"

            if "gguf" in backbone_config['repo'].lower() and backbone_device == "cuda":
                backbone_device = "gpu"
            
            print(f"ğŸ“¦ Loading model...")
            print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")
            
            tts = VieNeuTTS(
                backbone_repo=backbone_config["repo"],
                backbone_device=backbone_device,
                codec_repo=codec_config["repo"],
                codec_device=codec_device,
                hf_token=custom_hf_token
            )

            # Perform LoRA Merge if needed (ONLY for Standard Backend)
            # For LMDeploy, we handled it above by saving to disk
            if is_merged_lora and custom_loading and not using_lmdeploy:
                yield (
                    f"ğŸ”„ Äang táº£i vÃ  merge LoRA adapter: {custom_model_id}...",
                    gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False), gr.update(),
                    gr.update(), gr.update(), gr.update(), gr.update()
                )
                try:
                    # 1. Load Adapter
                    tts.load_lora_adapter(custom_model_id.strip(), hf_token=custom_hf_token)
                    
                    # 2. Merge and Unload
                    # Check if backbone matches expected type for merge
                    if hasattr(tts, 'backbone') and hasattr(tts.backbone, 'merge_and_unload'):
                        print("   ğŸ”„ Merging LoRA into backbone...")
                        tts.backbone = tts.backbone.merge_and_unload()
                        
                        # Reset LoRA state so it behaves like a normal model
                        tts._lora_loaded = False 
                        tts._current_lora_repo = None
                        print("   âœ… Merged successfully!")
                    else:
                        print("   âš ï¸ Warning: Model does not support merge_and_unload, keeping adapter active.")
                        
                except Exception as e:
                     raise RuntimeError(f"Failed to merge LoRA: {e}")

            using_lmdeploy = False
        
        current_backbone = backbone_choice
        current_codec = codec_choice
        model_loaded = True
        
        # Success message with optimization info
        backend_name = "ğŸš€ LMDeploy (Optimized)" if using_lmdeploy else "ğŸ“¦ Standard"
        device_info = "cuda" if use_lmdeploy else (backbone_device if not use_lmdeploy else "N/A")
        
        streaming_support = "âœ… CÃ³" if backbone_config['supports_streaming'] else "âŒ KhÃ´ng"
        preencoded_note = "\nâš ï¸ Codec nÃ y cáº§n sá»­ dá»¥ng pre-encoded codes (.pt files)" if codec_config['use_preencoded'] else ""
        
        opt_info = ""
        if using_lmdeploy and hasattr(tts, 'get_optimization_stats'):
            stats = tts.get_optimization_stats()
            opt_info = (
                f"\n\nğŸ”§ Tá»‘i Æ°u hÃ³a:"
                f"\n  â€¢ Triton: {'âœ…' if stats['triton_enabled'] else 'âŒ'}"
                f"\n  â€¢ Max Batch Size (Default): {stats.get('max_batch_size', 'N/A')}"
                f"\n  â€¢ Reference Cache: {stats['cached_references']} voices"
                f"\n  â€¢ Prefix Caching: âœ…"
            )
        
        warning_msg = ""
        if lmdeploy_error_reason:
             warning_msg = (
                 f"\n\nâš ï¸ **Cáº£nh bÃ¡o:** KhÃ´ng thá»ƒ kÃ­ch hoáº¡t LMDeploy (Optimized Backend) do lá»—i sau:\n"
                 f"ğŸ‘‰ {lmdeploy_error_reason}\n"
                 f"ğŸ’¡ Há»‡ thá»‘ng Ä‘Ã£ tá»± Ä‘á»™ng chuyá»ƒn vá» cháº¿ Ä‘á»™ Standard (cháº­m hÆ¡n)."
             )

        success_msg = get_model_status_message()
        if warning_msg:
            success_msg += warning_msg
            
        # Prepare voice update
        try:
            # Get voices with descriptions for UI from SDK
            voices = tts.list_preset_voices()
        except Exception:
            voices = []

        has_voices = len(voices) > 0
        
        if has_voices:
            default_v = tts._default_voice
            
            # Helper to get values list
            is_tuple = (len(voices) > 0 and isinstance(voices[0], tuple))
            voice_values = [v[1] for v in voices] if is_tuple else voices
            
            if not default_v and voice_values:
                 default_v = voice_values[0]

            # Ensure default_v is in the list and selected correctly
            if default_v and default_v not in voice_values:
                if is_tuple:
                    # Try to find a nice description if possible, else use ID
                    voices.append((default_v, default_v))
                else:
                    voices.append(default_v)
            
            # Sort voices by name/label for better UX
            if is_tuple:
                voices.sort(key=lambda x: str(x[0]))
            else:
                voices.sort()

            voice_update = gr.update(choices=voices, value=default_v, interactive=True)
            
            # Show Standard Tabs
            tab_p = gr.update(visible=True)
            tab_c = gr.update(visible=True)
            tab_sel = gr.update(selected="preset_mode")
            mode_state = "preset_mode"
        else:
            # Missing voices.json case
            msg = "âš ï¸ KhÃ´ng tÃ¬m tháº¥y file voices.json. Vui lÃ²ng dÃ¹ng Tab Voice Cloning."
            voice_update = gr.update(choices=[msg], value=msg, interactive=False)
            
            # Show Preset Tab (to see message) and Custom Tab
            tab_p = gr.update(visible=True)
            tab_c = gr.update(visible=True)
            tab_sel = gr.update(selected="preset_mode")
            mode_state = "preset_mode"

        yield (
            success_msg,
            gr.update(interactive=True), # btn_generate
            gr.update(interactive=True), # btn_load
            gr.update(interactive=False), # btn_stop
            voice_update,
            tab_p, tab_c, tab_sel, mode_state
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        model_loaded = False
        using_lmdeploy = False

        if "$env:CUDA_PATH" in str(e):
            yield (
                "âŒ Lá»—i khi táº£i model: KhÃ´ng tÃ¬m tháº¥y biáº¿n mÃ´i trÆ°á»ng CUDA_PATH. Vui lÃ²ng cÃ i Ä‘áº·t NVIDIA GPU Computing Toolkit (https://developer.nvidia.com/cuda/toolkit)",
                gr.update(interactive=False),
                gr.update(interactive=True),
                gr.update(interactive=False),
                gr.update(),
                gr.update(), gr.update(), gr.update(), gr.update()
            )
        else: 
            yield (
                f"âŒ Lá»—i khi táº£i model: {str(e)}",
                gr.update(interactive=False),
                gr.update(interactive=True),
                gr.update(interactive=False),
                gr.update(),
                gr.update(), gr.update(), gr.update(), gr.update()
            )


# --- 2. DATA & HELPERS ---

def synthesize_speech(text: str, voice_choice: str, custom_audio, custom_text: str, 
                      mode_tab: str, generation_mode: str, use_batch: bool, max_batch_size_run: int,
                      temperature: float, max_chars_chunk: int):
    """Synthesis with optimization support and max batch size control"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy
    
    if not model_loaded or tts is None:
        yield None, "âš ï¸ Vui lÃ²ng táº£i model trÆ°á»›c!"
        return
    
    if not text or text.strip() == "":
        yield None, "âš ï¸ Vui lÃ²ng nháº­p vÄƒn báº£n!"
        return
    
    raw_text = text.strip()
    
    codec_config = CODEC_CONFIGS[current_codec]
    use_preencoded = codec_config['use_preencoded']
    
    
    # Setup Reference
    yield None, "ğŸ“„ Äang xá»­ lÃ½ Reference..."
    
    try:
        ref_codes = None
        ref_text_raw = ""
        
        if mode_tab == "preset_mode":
            if not voice_choice:
                raise ValueError("Vui lÃ²ng chá»n giá»ng máº«u.")
            if "âš ï¸" in voice_choice:
                raise ValueError("KhÃ´ng cÃ³ giá»ng máº«u kháº£ dá»¥ng. Vui lÃ²ng chuyá»ƒn sang Tab Voice Cloning.")
            
            # Use SDK method - handles caching and JSON internally
            voice_data = tts.get_preset_voice(voice_choice)
            ref_codes = voice_data['codes']
            ref_text_raw = voice_data['text']
            
        elif mode_tab == "custom_mode":
            # Reference from Custom Cloning UI
            if custom_audio is None:
                 raise ValueError("Vui lÃ²ng upload file Audio máº«u (Reference Audio)!")
            if not custom_text or not custom_text.strip():
                 raise ValueError("Vui lÃ²ng nháº­p ná»™i dung vÄƒn báº£n cá»§a Audio máº«u (Reference Text)!")
            
            ref_text_raw = custom_text.strip()
            ref_codes = tts.encode_reference(custom_audio)
            
        else:
            raise ValueError(f"Unknown mode: {mode_tab}")

        # Ensure numpy for inference
        if isinstance(ref_codes, torch.Tensor):
            ref_codes = ref_codes.cpu().numpy()

    except Exception as e:
        yield None, f"âŒ Lá»—i xá»­ lÃ½ Reference Audio: {str(e)}"
        return
    
    text_chunks = split_text_into_chunks(raw_text, max_chars=max_chars_chunk)
    total_chunks = len(text_chunks)
    
    # === STANDARD MODE ===
    if generation_mode == "Standard (Má»™t láº§n)":
        backend_name = "LMDeploy" if using_lmdeploy else "Standard"
        batch_info = " (Batch Mode)" if use_batch and using_lmdeploy and total_chunks > 1 else ""
        batch_size_info = ""
        if use_batch and using_lmdeploy and hasattr(tts, 'max_batch_size'):
            batch_size_info = f" [Max batch: {tts.max_batch_size}]"
        
        yield None, f"ğŸš€ Báº¯t Ä‘áº§u tá»•ng há»£p {backend_name}{batch_info}{batch_size_info} ({total_chunks} Ä‘oáº¡n)..."
        
        all_wavs = []
        sr = 24000
        start_time = time.time()
        
        try:
            if use_batch and using_lmdeploy and hasattr(tts, 'infer_batch') and total_chunks > 1:
                num_batches = (total_chunks + max_batch_size_run - 1) // max_batch_size_run
                yield None, f"âš¡ Xá»­ lÃ½ {num_batches} mini-batch(es) (max {max_batch_size_run} Ä‘oáº¡n/batch)..."
                
                chunk_wavs = tts.infer_batch(
                    text_chunks, 
                    ref_codes=ref_codes, 
                    ref_text=ref_text_raw,
                    max_batch_size=max_batch_size_run,
                    temperature=temperature
                )
                
                for chunk_wav in chunk_wavs:
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_wavs.append(chunk_wav)
            else:
                # Sequential processing with progress updates
                for i, chunk in enumerate(text_chunks):
                    yield None, f"â³ Äang xá»­ lÃ½ Ä‘oáº¡n {i+1}/{total_chunks}..."
                    
                    chunk_wav = tts.infer(
                        chunk, 
                        ref_codes=ref_codes, 
                        ref_text=ref_text_raw,
                        temperature=temperature
                    )
                    
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_wavs.append(chunk_wav)
            
            if not all_wavs:
                yield None, "âŒ KhÃ´ng sinh Ä‘Æ°á»£c audio nÃ o."
                return
            
            yield None, "ğŸ’¾ Äang ghÃ©p file vÃ  lÆ°u..."
            
            # Use utility function for joining with silence/crossfade
            # Default silence=0.15s to match SDK
            final_wav = join_audio_chunks(all_wavs, sr=sr, silence_p=0.15)
            
            # Save to specific folder
            from datetime import datetime
            
            output_dir = "output_audio"
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(output_dir, f"tts_output_{timestamp}.wav")
            sf.write(output_path, final_wav, sr)
            
            # Verify file was written
            if os.path.exists(output_path):
                file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
                print(f"âœ… File saved successfully: {output_path} ({file_size_mb:.2f} MB)")
            else:
                print(f"âš ï¸ WARNING: File not found after write: {output_path}")
            
            abs_output_path = os.path.abspath(output_path)
            
            process_time = time.time() - start_time
            speed_info = f", Tá»‘c Ä‘á»™: {len(final_wav)/sr/process_time:.2f}x realtime" if process_time > 0 else ""
            
            yield output_path, f"âœ… HoÃ n táº¥t! (Thá»i gian: {process_time:.2f}s{speed_info})\nğŸ“ File Ä‘Ã£ lÆ°u táº¡i: {abs_output_path}"
            cleanup_gpu_memory()
            return

        except torch.cuda.OutOfMemoryError as e:
            cleanup_gpu_memory()
            yield None, (
                f"âŒ GPU háº¿t VRAM! HÃ£y thá»­:\n"
                f"â€¢ Giáº£m Max Batch Size (hiá»‡n táº¡i: {tts.max_batch_size if hasattr(tts, 'max_batch_size') else 'N/A'})\n"
                f"â€¢ Giáº£m Ä‘á»™ dÃ i vÄƒn báº£n\n\n"
                f"Chi tiáº¿t: {str(e)}"
            )
            return
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            cleanup_gpu_memory()
            yield None, f"âŒ Lá»—i Standard Mode: {str(e)}"
            return
    
    # === STREAMING MODE ===
    else:
        sr = 24000
        crossfade_samples = int(sr * 0.03)
        audio_queue = queue.Queue(maxsize=100)
        PRE_BUFFER_SIZE = 3
        
        end_event = threading.Event()
        error_event = threading.Event()
        error_msg = ""
        
        def producer_thread():
            nonlocal error_msg
            try:
                previous_tail = None
                
                for i, chunk_text in enumerate(text_chunks):
                    stream_gen = tts.infer_stream(
                        chunk_text, 
                        ref_codes=ref_codes, 
                        ref_text=ref_text_raw,
                        temperature=temperature
                    )
                    
                    for part_idx, audio_part in enumerate(stream_gen):
                        if audio_part is None or len(audio_part) == 0:
                            continue
                        
                        if previous_tail is not None and len(previous_tail) > 0:
                            overlap = min(len(previous_tail), len(audio_part), crossfade_samples)
                            if overlap > 0:
                                fade_out = np.linspace(1.0, 0.0, overlap, dtype=np.float32)
                                fade_in = np.linspace(0.0, 1.0, overlap, dtype=np.float32)
                                
                                blended = (audio_part[:overlap] * fade_in + 
                                         previous_tail[-overlap:] * fade_out)
                                
                                processed = np.concatenate([
                                    previous_tail[:-overlap] if len(previous_tail) > overlap else np.array([]),
                                    blended,
                                    audio_part[overlap:]
                                ])
                            else:
                                processed = np.concatenate([previous_tail, audio_part])
                            
                            tail_size = min(crossfade_samples, len(processed))
                            previous_tail = processed[-tail_size:].copy()
                            output_chunk = processed[:-tail_size] if len(processed) > tail_size else processed
                        else:
                            tail_size = min(crossfade_samples, len(audio_part))
                            previous_tail = audio_part[-tail_size:].copy()
                            output_chunk = audio_part[:-tail_size] if len(audio_part) > tail_size else audio_part
                        
                        if len(output_chunk) > 0:
                            audio_queue.put((sr, output_chunk))
                
                if previous_tail is not None and len(previous_tail) > 0:
                    audio_queue.put((sr, previous_tail))
                    
            except Exception as e:
                import traceback
                traceback.print_exc()
                error_msg = str(e)
                error_event.set()
            finally:
                end_event.set()
                audio_queue.put(None)
        
        threading.Thread(target=producer_thread, daemon=True).start()
        
        yield (sr, np.zeros(int(sr * 0.05))), "ğŸ“„ Äang buffering..."
        
        pre_buffer = []
        while len(pre_buffer) < PRE_BUFFER_SIZE:
            try:
                item = audio_queue.get(timeout=5.0)
                if item is None:
                    break
                pre_buffer.append(item)
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"âŒ Lá»—i: {error_msg}"
                    return
                break
        
        full_audio_buffer = []
        backend_info = "ğŸš€ LMDeploy" if using_lmdeploy else "ğŸ“¦ Standard"
        for sr, audio_data in pre_buffer:
            full_audio_buffer.append(audio_data)
            yield (sr, audio_data), f"ğŸ”Š Äang phÃ¡t ({backend_info})..."
        
        while True:
            try:
                item = audio_queue.get(timeout=0.05)
                if item is None:
                    break
                sr, audio_data = item
                full_audio_buffer.append(audio_data)
                yield (sr, audio_data), f"ğŸ”Š Äang phÃ¡t ({backend_info})..."
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"âŒ Lá»—i: {error_msg}"
                    break
                if end_event.is_set() and audio_queue.empty():
                    break
                continue
        
        if full_audio_buffer:
            final_wav = np.concatenate(full_audio_buffer)
            
            # Save to specific folder
            from datetime import datetime
            
            output_dir = "output_audio"
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(output_dir, f"tts_stream_{timestamp}.wav")
            sf.write(output_path, final_wav, sr)
            
            abs_output_path = os.path.abspath(output_path)
            
            yield output_path, f"âœ… HoÃ n táº¥t Streaming!\nğŸ“ File Ä‘Ã£ lÆ°u táº¡i: {abs_output_path}"
            
            cleanup_gpu_memory()

# --- 3. UI SETUP ---
theme = gr.themes.Soft(
    primary_hue="indigo",
    secondary_hue="cyan",
    neutral_hue="slate",
    font=[gr.themes.GoogleFont('Inter'), 'ui-sans-serif', 'system-ui'],
).set(
    button_primary_background_fill="linear-gradient(90deg, #6366f1 0%, #0ea5e9 100%)",
    button_primary_background_fill_hover="linear-gradient(90deg, #4f46e5 0%, #0284c7 100%)",
)

css = """
.container { max-width: 1400px; margin: auto; }
"""

with gr.Blocks(theme=theme, css=css, title="VieNeu-TTS") as demo:
    with gr.Column(elem_classes="container"):
        gr.Markdown("# ğŸ¦œ VieNeu-TTS Studio")
        
        # Model Configuration
        with gr.Group():
            with gr.Row():
                backbone_select = gr.Dropdown(
                    list(BACKBONE_CONFIGS.keys()) + ["Custom Model"], 
                    value="VieNeu-TTS-0.3B (GGUF)", 
                    label="ğŸ¦œ Backbone"
                )
                codec_select = gr.Dropdown(
                    list(CODEC_CONFIGS.keys()), 
                    value="NeuCodec (Distill)", 
                    label="ğŸµ Codec"
                )
                device_choice = gr.Radio(
                    get_available_devices(), 
                    value="Auto", 
                    label="ğŸ–¥ï¸ Device"
                )
            
            force_lmdeploy = gr.Checkbox(
                value=False, 
                label="âš¡ Enable LMDeploy (GPU only)",
                info="Enables optimizations for faster generation"
            )
            
            with gr.Row(visible=False) as custom_model_group:
                custom_backbone_model_id = gr.Textbox(
                    label="ğŸ“¦ Custom Model ID",
                    placeholder="username/model-name",
                    scale=2
                )
                custom_backbone_hf_token = gr.Textbox(
                    label="ğŸ”‘ HF Token",
                    type="password",
                    scale=1
                )
                custom_backbone_base_model = gr.Dropdown(
                    [k for k in BACKBONE_CONFIGS.keys() if "gguf" not in k.lower()],
                    label="ğŸ”— Base Model (for LoRA)",
                    value="VieNeu-TTS-0.3B (GPU)",
                    visible=False,
                    scale=1
                )
            
            btn_load = gr.Button("ğŸ”„ Táº£i Model", variant="primary")
            model_status = gr.Markdown("â³ ChÆ°a táº£i model.")
        
        # Input/Output
        with gr.Row():
            with gr.Column(scale=3):
                text_input = gr.Textbox(
                    label="VÄƒn báº£n",
                    lines=4,
                    value="Xin chÃ o, Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» viá»‡c tá»•ng há»£p giá»ng nÃ³i tiáº¿ng Viá»‡t."
                )
                
                with gr.Tabs() as tabs:
                    with gr.TabItem("ğŸ‘¤ Preset", id="preset_mode") as tab_preset:
                        voice_select = gr.Dropdown(choices=[], value=None, label="Giá»ng máº«u")
                    
                    with gr.TabItem("ğŸ¦œ Voice Cloning", id="custom_mode") as tab_custom:
                        custom_audio = gr.Audio(label="Audio giá»ng máº«u", type="filepath")
                        custom_text = gr.Textbox(label="Ná»™i dung audio máº«u")
                
                generation_mode = gr.Radio(
                    ["Standard (Má»™t láº§n)", "Streaming (Real-time)"],
                    value="Standard (Má»™t láº§n)",
                    label="Cháº¿ Ä‘á»™ sinh"
                )
                
                with gr.Row():
                    use_batch = gr.Checkbox(value=True, label="âš¡ Batch Processing")
                    max_batch_size_run = gr.Slider(
                        minimum=1, maximum=128, value=64, step=1,
                        label="ğŸ“Š Batch Size"
                    )
                
                with gr.Accordion("âš™ï¸ Advanced Settings", open=False):
                    with gr.Row():
                        temperature_slider = gr.Slider(
                            minimum=0.1, maximum=1.5, value=1.0, step=0.1,
                            label="ğŸŒ¡ï¸ Temperature"
                        )
                        max_chars_chunk_slider = gr.Slider(
                            minimum=64, maximum=512, value=256, step=16,
                            label="ğŸ“ Max Chars per Chunk"
                        )
                
                current_mode_state = gr.State("preset_mode")
                
                with gr.Row():
                    btn_generate = gr.Button("ğŸµ Báº¯t Ä‘áº§u", variant="primary", scale=2, interactive=False)
                    btn_stop = gr.Button("â¹ï¸ Dá»«ng", variant="stop", scale=1, interactive=False)
            
            with gr.Column(scale=2):
                audio_output = gr.Audio(label="Káº¿t quáº£", type="filepath", autoplay=True)
                status_output = gr.Textbox(label="Tráº¡ng thÃ¡i", lines=3, max_lines=10)
        
        # Event Handlers
        def on_backbone_change(choice):
            return gr.update(visible=(choice == "Custom Model"))
        
        backbone_select.change(
            on_backbone_change,
            inputs=[backbone_select],
            outputs=[custom_model_group]
        )
        
        tab_preset.select(lambda: "preset_mode", outputs=current_mode_state)
        tab_custom.select(lambda: "custom_mode", outputs=current_mode_state)
        
        btn_load.click(
            fn=load_model,
            inputs=[
                backbone_select, codec_select, device_choice, force_lmdeploy,
                custom_backbone_model_id, custom_backbone_base_model, custom_backbone_hf_token
            ],
            outputs=[
                model_status, btn_generate, btn_load, btn_stop, voice_select,
                tab_preset, tab_custom, tabs, current_mode_state
            ]
        )
        
        generate_event = btn_generate.click(
            fn=synthesize_speech,
            inputs=[
                text_input, voice_select, custom_audio, custom_text, current_mode_state,
                generation_mode, use_batch, max_batch_size_run,
                temperature_slider, max_chars_chunk_slider
            ],
            outputs=[audio_output, status_output]
        )
        
        btn_generate.click(lambda: gr.update(interactive=True), outputs=btn_stop)
        generate_event.then(lambda: gr.update(interactive=False), outputs=btn_stop)
        
        btn_stop.click(fn=None, cancels=[generate_event])
        btn_stop.click(lambda: (None, "â¹ï¸ ÄÃ£ dá»«ng."), outputs=[audio_output, status_output])
        btn_stop.click(lambda: gr.update(interactive=False), outputs=btn_stop)
        
        demo.load(fn=restore_ui_state, outputs=[model_status, btn_generate, btn_stop])

if __name__ == "__main__":
    server_name = os.getenv("GRADIO_SERVER_NAME", "127.0.0.1")
    server_port = int(os.getenv("GRADIO_SERVER_PORT", "7860"))
    share = env_bool("GRADIO_SHARE", default=False)
    
    print(f"ğŸš€ Launching app on http://{server_name}:{server_port}")
    print(f"ğŸ“¡ API is automatically enabled")
    print(f"ğŸ“‹ API endpoints: /api/predict, /api/load_model, /api/synthesize_speech")
    
    demo.queue().launch(
        server_name=server_name, 
        server_port=server_port, 
        share=share
        # Note: show_api is removed in Gradio 6.0 - API is always available
    )
